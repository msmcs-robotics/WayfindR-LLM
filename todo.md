So what I'm trying to build is a robot that acts as a tour guide inside a building and it will have components like a Raspberry Pi with Ubuntu and Ross 2 and using lidar and other sensors for indoor navigation. And then on that robot will be an Android tablet with an Android app for users to talk to and interact with and basically have the ability to ask questions about the place and also be able to have the robot navigate or guide them to different rooms or everything like that. The idea is to the android app and onboard tools of A Android Tablet to get what the user is saying and turn it into text and then send it to a remote AI agent for parsing what their intent would be like through an MCP server preferably Python fast MCP and then also being able to execute functions like moving the robot and more. So basically this entire project needs reconfiguring restructuring so that I can build an app to have endpoints that receive incoming messages and telemetry from the android app such as z A robot has sent a new message and the agent needs to handle it or a robot is sending new telemetry and there's errors or notifications. And then I would eventually like to be able to scale this to be able to use multiple agents. And I already have done a lot of research and development on LL miss multi agent systems which is precisely why I'm trying to develop this to be a much more compact version of just the essentials of having an MCP server preferably Python fast MCP and then also having a fast API component to serve those API endpoints and then I already have code for being able to get Olama up and running and have models available and everything like that. Because I plan to run this application either on the HPC cluster or on a local development server and then bridge the gap to the HPC cluster but for now let's just try to run it on my laptop and if you need to run the scripts and go to the HPC cluster as well. Then I would also like to have a dashboard to stream all telemetry like from qdrant and Postgresql as seen before and then I would have another page or HTML file that will show live map if that makes sense because I should have the map files available from being generated in Ross 2 and it D be cool if I could show where agents are on the map live from the computer but don't worry about making that live map right now just worry about making a concise version of what I had made and don't worry about making the simulation engine don't worry about making the satellite constellation just worry about making that MCP server and being able to receive data on endpoints to stream to qdrant and stream messages to post graysql and everything and you should be able to view how things are handled in docker and everything. It does do this all makes sense and if you need to look at all this code for context it is stored in the directory on my local machine /home/devel/cars_demo_13

And this will contain all the code for how to set up an MCP chat server how to stream data and to queue and post grad SQL and how to show live updates to a dashboard and everything and again don't worry about making that live map that will be a feature to be implemented later And don't worry about making that simulation engine because that simulation engine with all those agents and graphing and everything and GUI is just a demo so that everything can be implemented into a system like this and then especially don't worry about the GPS constellation and everything. And you should be able to find the scripts that you should be able to copy directly into this project for managing O llama and port forwards and everything does this make sense.



Please continue with any tasks that you left off on.

yes and i guess i would like to have a chat on the dashboard where a user can ask for reports on the system or ask questions about agents and then also manually move agents to different locations or tell agents to say thing or perform other actions. doe sthis make sense? make sure this type of chat is different from agents as this will be an "operator" chat where the operator can interact with the system and manage agents, but the LLM does not tell the operator to go to waypoints or perform actions or anything like that. it's more of a management tool for the operator to interact with the system and manage agents.